# -*- coding: utf-8 -*-
"""Proyecto_01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U_P5QxzjOuK2r8i3TMYlTdeIrfuPgov2
"""
import streamlit as st # Biblioteca principal para crear aplicaciones web interactivas.
import os # Módulo para interactuar con el sistema operativo (rutas de archivos, directorios).
import docx # Librería para trabajar con archivos Microsoft Word (.docx).
import openpyxl # Librería para trabajar con archivos Microsoft Excel (.xlsx).
from pptx import Presentation # Para trabajar con archivos Microsoft PowerPoint (.pptx).
import PyPDF2 # Para operaciones básicas con PDFs (metadatos, detección de formularios).
import pdfplumber # Para una extracción de texto y análisis más avanzado de PDFs.
import pandas as pd # Biblioteca para manipulación y análisis de datos (DataFrames).
import nltk # Natural Language Toolkit, para procesamiento de lenguaje natural.
from nltk.corpus import stopwords # Palabras muy comunes que suelen eliminarse en PLN.
from nltk.tokenize import word_tokenize # Para dividir texto en palabras.

# Importaciones para Redes Neuronales (Transformers)
from transformers import pipeline # Herramienta de Hugging Face para usar modelos pre-entrenados fácilmente.
from sentence_transformers import SentenceTransformer # Para generar embeddings (representaciones vectoriales) de frases.
from sklearn.cluster import MiniBatchKMeans # Algoritmo de clustering para agrupar documentos.
from sklearn.metrics import silhouette_score # Para evaluar la calidad del clustering (opcional).

# --- Configuración Inicial y Descargas de NLTK/spaCy ---
# Estas líneas aseguran que los recursos necesarios para NLTK y spaCy estén disponibles.
# Se ejecutan solo una vez si los recursos no existen.
try:
    nltk.data.find('corpora/stopwords')
except LookupError: # Corrected from nltk.downloader.DownloadError
    nltk.download('stopwords')
try:
    nltk.data.find('tokenizers/punkt')
except LookupError: # Corrected from nltk.downloader.DownloadError
    nltk.download('punkt')

# Cargar modelo de spaCy para procesamiento básico (si se sigue usando)
# python -m spacy download es_core_news_sm
try:
    import spacy
    nlp = spacy.load("es_core_news_sm")
except OSError:
    st.warning("El modelo 'es_core_news_sm' de spaCy no está descargado. Descargándolo...")
    import spacy.cli
    spacy.cli.download("es_core_news_sm")
    import spacy
    nlp = spacy.load("es_core_news_sm")

# --- Inicialización de Modelos de Redes Neuronales (Global para evitar recargas) ---
# Usamos st.cache_resource para que Streamlit cargue estos modelos solo una vez
# y los mantenga en memoria entre ejecuciones del script, optimizando el rendimiento.

@st.cache_resource
def load_summarizer_model():
    """Carga un modelo de resumen pre-entrenado de Hugging Face."""
    # Se reemplaza por un modelo público diferente debido a problemas de acceso/autenticación con el anterior.
    # Dado que persisten los errores de carga, se desactiva la carga del modelo de resumen
    # y se retorna una función dummy para evitar el crash.
    try:
        return pipeline("summarization", model="mrm8488/distilbart-cnn-spanish-fine-tuned", tokenizer="mrm8488/distilbart-cnn-spanish-fine-tuned", framework="pt")
    except Exception as e:
        st.error(f"Error al cargar el modelo de resumen de Hugging Face: {e}. La funcionalidad de resumen IA no estará disponible.")
        # Retornar una función dummy que simula el comportamiento esperado pero con un mensaje de error
        return lambda text, max_length, min_length, do_sample: [{'summary_text': 'IA Summarization no disponible debido a un error de carga del modelo.'}]

@st.cache_resource
def load_sentence_transformer_model():
    """Carga un modelo para generar embeddings de frases/documentos."""
    # 'hiiamsid/sentence_similarity_spanish_es' es un buen modelo para embeddings semánticos en español.
    return SentenceTransformer('hiiamsid/sentence_similarity_spanish_es')

# Cargamos los modelos al inicio de la aplicación
summarizer = load_summarizer_model()
sentence_model = load_sentence_transformer_model()

st.set_page_config(layout="wide") # Configura el diseño de la página de Streamlit a ancho completo.
st.title("Clasificador y Agrupador Inteligente de Documentos con IA") # Título de la aplicación.

# --- Funciones de procesamiento de archivos (similar al anterior, con pequeñas mejoras) ---

def get_file_info(filepath):
    """Obtiene información básica de un archivo y prepara un diccionario."""
    filename = os.path.basename(filepath)
    file_extension = os.path.splitext(filename)[1].lower()
    file_size = os.path.getsize(filepath)
    return {
        "Nombre": filename,
        "Ruta": filepath,
        "Extensión": file_extension,
        "Tamaño (Bytes)": file_size,
        "Tipo Clasificado": "Desconocido",
        "Resumen Generado (IA)": "",
        "PDF con Firma": "N/A",
        "PDF solo Imágenes": "N/A",
        "Texto Completo para IA": "", # Almacenará el texto extraído para modelos de IA
        "Grupo": "Sin agrupar"
    }

def process_word(filepath):
    """Procesa archivos Word (.docx) y extrae su texto."""
    try:
        doc = docx.Document(filepath)
        full_text = []
        for para in doc.paragraphs:
            full_text.append(para.text)
        text_content = "\n".join(full_text)
        return "Word", text_content
    except Exception as e:
        return "Word", f"Error al leer Word: {e}"

def process_excel(filepath):
    """Procesa archivos Excel (.xlsx) y extrae un resumen de su contenido."""
    try:
        workbook = openpyxl.load_workbook(filepath)
        sheet = workbook.active
        summary = f"Columnas: {sheet.max_column}, Filas: {sheet.max_row}\n"
        for i in range(1, min(6, sheet.max_row + 1)): # Primeras 5 filas
            row_values = [str(cell.value) for cell in sheet[i] if cell.value is not None]
            if row_values:
                summary += ", ".join(row_values[:5]) + "...\n"
        return "Excel", summary
    except Exception as e:
        return "Excel", f"Error al leer Excel: {e}"

def process_powerpoint(filepath):
    """Procesa archivos PowerPoint (.pptx) y extrae su texto."""
    try:
        prs = Presentation(filepath)
        full_text = []
        for slide in prs.slides:
            for shape in slide.shapes:
                if hasattr(shape, "text_frame") and shape.text_frame:
                    full_text.append(shape.text_frame.text)
        text_content = "\n".join(full_text)
        return "PowerPoint", text_content
    except Exception as e:
        return "PowerPoint", f"Error al leer PowerPoint: {e}"

def process_pdf(filepath):
    """Procesa archivos PDF: extrae texto, detecta firma (heurística) e imágenes."""
    full_text_list = []
    is_signed = "No"
    is_image_only = True # Asumimos que es solo imágenes hasta que encontramos texto

    try:
        with pdfplumber.open(filepath) as pdf:
            for page in pdf.pages:
                text_page = page.extract_text()
                if text_page and text_page.strip():
                    full_text_list.append(text_page)
                    is_image_only = False # Hay texto

            # Heurística para firma digital (PyPDF2)
            reader = PyPDF2.PdfReader(filepath)
            if '/AcroForm' in reader.trailer and '/Fields' in reader.trailer['/AcroForm']:
                for field in reader.trailer['/AcroForm']['/Fields']:
                    if isinstance(field, PyPDF2.generic.ReferenceObject):
                        field_obj = reader.get_object(field)
                        if '/FT' in field_obj and field_obj['/FT'] == '/Sig':
                            is_signed = "Sí"
                            break

        full_text = "\n".join(full_text_list)
        return "PDF", full_text, is_signed, "Sí" if is_image_only and not full_text.strip() else "No"
    except Exception as e:
        return "PDF", f"Error al leer PDF: {e}", "N/A", "N/A"

def generate_summary_with_ai(text):
    """Genera un resumen usando el modelo de Transformer de Hugging Face."""
    if not text or len(text.strip()) < 50: # Mínimo de caracteres para que el resumen tenga sentido
        return "No hay suficiente texto para generar un resumen."
    try:
        # El modelo de resumen tiene un límite de tokens. Truncamos el texto si es muy largo.
        # Ajusta max_length y min_length según necesites.
        summarized = summarizer(text[:1000], max_length=150, min_length=50, do_sample=False)
        return summarized[0]['summary_text']
    except Exception as e:
        # En caso de que el modelo haya fallado en la carga, el 'summarizer' es la función dummy
        # que ya devuelve el mensaje de error.
        if 'IA Summarization no disponible' in summarizer(text, 0, 0, False)[0]['summary_text']:
            return summarizer(text, 0, 0, False)[0]['summary_text']
        return f"Error al generar resumen con IA: {e}"

def clean_text_for_embedding(text):
    """Limpia el texto para el embedding: minúsculas, elimina stop words y puntuación."""
    text = text.lower()
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('spanish'))
    # Filtra solo palabras alfabéticas y no stop words
    cleaned_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]
    return " ".join(cleaned_tokens)

# --- Streamlit UI ---

# Sección de carga de archivos en la barra lateral.
st.sidebar.header("Carga tu Carpeta o Archivo ZIP")
# st.help(st.file_uploader) # Para ver la documentación de file_uploader

# Opción para cargar un archivo ZIP (para entornos de nube)
uploaded_file = st.sidebar.file_uploader("Sube un archivo ZIP de tus documentos", type="zip")
# O la opción de ruta local (solo funcional si la app se ejecuta localmente)
local_folder_path = st.sidebar.text_input("O ingresa la ruta de una carpeta local (ej: C:/mis_docs)", "")

processed_dfs = [] # Lista para almacenar DataFrames procesados, en caso de múltiples cargas

if uploaded_file:
    # Lógica para descomprimir y procesar el ZIP
    import zipfile
    import tempfile

    st.info("Descomprimiendo archivo ZIP...")
    with tempfile.TemporaryDirectory() as tmpdir:
        with zipfile.ZipFile(uploaded_file, 'r') as zip_ref:
            zip_ref.extractall(tmpdir)

        st.success(f"Archivo ZIP '{uploaded_file.name}' descomprimido en una carpeta temporal.")
        st.subheader(f"Procesando documentos de '{uploaded_file.name}'")

        all_files_info = []
        # Recorrer la carpeta temporal y subcarpetas
        for root, _, files in os.walk(tmpdir):
            for file in files:
                filepath = os.path.join(root, file)
                process_single_file(filepath, all_files_info)

        processed_dfs.append(pd.DataFrame(all_files_info))

elif local_folder_path and st.sidebar.button("Escanear Carpeta Local"):
    if not os.path.isdir(local_folder_path):
        st.error("La ruta especificada no es una carpeta válida.")
    else:
        st.subheader(f"Escaneando carpeta: `{local_folder_path}`")
        all_files_info = []

        # Recorrer la carpeta local y subcarpetas
        for root, _, files in os.walk(local_folder_path):
            for file in files:
                filepath = os.path.join(root, file)
                process_single_file(filepath, all_files_info)

        processed_dfs.append(pd.DataFrame(all_files_info))

def process_single_file(filepath, all_files_info_list):
    """
    Función auxiliar para procesar un solo archivo y añadir su info a la lista.
    Separado para ser llamado tanto desde ZIP como desde ruta local.
    """
    file_info = get_file_info(filepath)
    text_content = "" # Texto extraído para el resumen y embeddings

    if file_info["Extensión"] == ".docx":
        file_info["Tipo Clasificado"], text_content = process_word(filepath)
    elif file_info["Extensión"] == ".xlsx":
        file_info["Tipo Clasificado"], text_content = process_excel(filepath)
    elif file_info["Extensión"] == ".pptx":
        file_info["Tipo Clasificado"], text_content = process_powerpoint(filepath)
    elif file_info["Extensión"] == ".pdf":
        file_info["Tipo Clasificado"], text_content, \
        file_info["PDF con Firma"], file_info["PDF solo Imágenes"] = process_pdf(filepath)
    else:
        # Para otros tipos de archivos, se puede intentar leer como texto plano o ignorar
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                text_content = f.read()
            file_info["Tipo Clasificado"] = "Texto Plano"
        except (UnicodeDecodeError, IsADirectoryError):
            file_info["Tipo Clasificado"] = "Binario/Desconocido"
            text_content = "" # No hay texto legible

    file_info["Texto Completo para IA"] = text_content # Almacena el texto para la IA

    # Generar resumen con IA si hay suficiente texto
    if file_info["Tipo Clasificado"] not in ["Binario/Desconocido", "Excel"] and text_content.strip():
        file_info["Resumen Generado (IA)"] = generate_summary_with_ai(text_content)
    else:
        file_info["Resumen Generado (IA)"] = "No aplicable o texto insuficiente para IA."

    all_files_info_list.append(file_info)

# Si se procesaron archivos, mostrar resultados.
if processed_dfs:
    df_final = pd.concat(processed_dfs, ignore_index=True)
    st.success(f"Se encontraron y procesaron {len(df_final)} archivos.")

    st.subheader("Archivos Encontrados y Clasificados")
    st.dataframe(df_final.drop(columns=["Texto Completo para IA"])) # No mostrar el texto completo en la tabla

    # --- Agrupación de Documentos (Clustering con Embeddings de Red Neuronal) ---
    st.subheader("Agrupación Inteligente de Documentos (con Embeddings Semánticos)")

    # Filtramos solo documentos con texto significativo para el clustering
    docs_for_clustering_df = df_final[df_final["Texto Completo para IA"].apply(lambda x: x.strip() != "")].copy()

    if len(docs_for_clustering_df) >= 2: # Mínimo 2 documentos para agrupar
        # Generar embeddings de los documentos usando el modelo de Transformer
        st.info("Generando embeddings semánticos para los documentos...")
        corpus_sentences = docs_for_clustering_df["Texto Completo para IA"].apply(clean_text_for_embedding).tolist()
        # Truncar textos largos para evitar errores en el modelo de embedding si son excesivos.
        corpus_sentences = [text[:512] for text in corpus_sentences] # Típico límite de tokens para muchos modelos
        document_embeddings = sentence_model.encode(corpus_sentences, show_progress_bar=True)

        st.success("Embeddings generados.")

        # Número de clusters
        # Permite al usuario elegir el número de grupos, con un rango sensato.
        max_clusters_allowed = max(2, len(docs_for_clustering_df) - 1)
        n_clusters = st.sidebar.slider("Número de grupos para clustering", 2, max_clusters_allowed, min(3, max_clusters_allowed))

        # Aplicar K-Means al final de la barra lateral para dejar el main clean
        if st.sidebar.button("Realizar Agrupación (Clustering)"):
            st.info(f"Realizando clustering en {len(docs_for_clustering_df)} documentos en {n_clusters} grupos...")
            kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=0, n_init='auto', batch_size=256)
            kmeans.fit(document_embeddings)
            clusters = kmeans.predict(document_embeddings)

            # Asignar los clusters de vuelta al DataFrame original
            df_final['Grupo'] = "Sin agrupar" # Inicializar todos a 'Sin agrupar'
            for i, original_idx in enumerate(docs_for_clustering_df.index):
                df_final.loc[original_idx, 'Grupo'] = f"Grupo {clusters[i] + 1}"

            st.success(f"Documentos agrupados en {n_clusters} categorías.")
            st.dataframe(df_final.drop(columns=["Texto Completo para IA"]))

            # Opcional: Evaluar la calidad del clustering (si hay suficientes clusters y puntos)
            if n_clusters > 1 and len(document_embeddings) >= n_clusters:
                try:
                    silhouette_avg = silhouette_score(document_embeddings, clusters)
                    st.info(f"Puntuación de silueta para la agrupación: {silhouette_avg:.2f} (cuanto más cerca de 1, mejor separación).")
                except Exception as e:
                    st.warning(f"No se pudo calcular la puntuación de silueta: {e}")

            # Descarga de resultados
            csv_data = df_final.to_csv(index=False).encode('utf-8')
            st.download_button(
                label="Descargar Resultados en CSV",
                data=csv_data,
                file_name="analisis_documentos_ia.csv",
                mime="text/csv",
            )
        else:
            st.info("Presiona 'Realizar Agrupación (Clustering)' en la barra lateral para agrupar los documentos.")
    else:
        st.warning("No hay suficientes documentos con contenido significativo para realizar la agrupación semántica (necesitas al menos 2).")

# Información sobre los modelos de IA
st.sidebar.markdown("---")
st.sidebar.info("Esta aplicación utiliza modelos de Transformers de Hugging Face para generación de resúmenes (modelo 'mrm8488/distilbart-cnn-spanish-fine-tuned' - **actualmente no disponible debido a errores de carga**) y embeddings semánticos para agrupación (modelo 'hiiamsid/sentence_similarity_spanish_es').")
st.sidebar.caption("La detección de firma digital en PDF es una heurística básica y no es infalible.")
